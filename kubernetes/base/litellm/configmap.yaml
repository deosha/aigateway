apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  labels:
    app: litellm
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Documentation: https://docs.litellm.ai/docs/proxy/configs
    #
    # Supported Providers:
    # - OpenAI (GPT-5, o3, o4-mini)
    # - Anthropic (Claude Opus 4.5, Sonnet 4.5, Haiku 4.5)
    # - Google (Gemini 3 Pro, 2.5 Pro/Flash)
    # - xAI (Grok 4, Grok 3)
    # - DeepSeek (V3.2, R1)
    # - AWS Bedrock (~100 models: Claude, Llama 4, Nova, Mistral, Titan, DeepSeek)
    # - Google Vertex AI (200+ models: Gemini, Claude, DeepSeek)
    # - Ollama (local models)
    #
    # NOTE: Azure OpenAI models excluded - require api_base env var which crashes startup if missing

    # ===========================================
    # Model Configuration (70 models)
    # ===========================================
    model_list:

      # ===========================================
      # OpenAI Models (8)
      # ===========================================
      # Docs: https://platform.openai.com/docs/models

      # GPT-5 Series (Flagship)
      - model_name: "gpt-5"
        litellm_params:
          model: "gpt-5"
          timeout: 300
        model_info:
          id: "gpt-5"
          mode: "chat"

      - model_name: "gpt-5.2"
        litellm_params:
          model: "gpt-5.2"
          timeout: 300
        model_info:
          id: "gpt-5.2"
          mode: "chat"

      - model_name: "gpt-5-mini"
        litellm_params:
          model: "gpt-5-mini"
          timeout: 120
        model_info:
          id: "gpt-5-mini"
          mode: "chat"

      # o-Series Reasoning Models
      - model_name: "o3"
        litellm_params:
          model: "o3"
          timeout: 600
        model_info:
          id: "o3"
          mode: "chat"

      - model_name: "o3-pro"
        litellm_params:
          model: "o3-pro"
          timeout: 900
        model_info:
          id: "o3-pro"
          mode: "chat"

      - model_name: "o4-mini"
        litellm_params:
          model: "o4-mini"
          timeout: 300
        model_info:
          id: "o4-mini"
          mode: "chat"

      # Legacy (still supported)
      - model_name: "gpt-4o"
        litellm_params:
          model: "gpt-4o"
          timeout: 300
        model_info:
          id: "gpt-4o"
          mode: "chat"

      - model_name: "gpt-4o-mini"
        litellm_params:
          model: "gpt-4o-mini"
          timeout: 120
        model_info:
          id: "gpt-4o-mini"
          mode: "chat"

      # ===========================================
      # Anthropic Models (7)
      # ===========================================
      # Docs: https://docs.anthropic.com/claude/docs/models-overview

      # Claude 4.5 Series (Latest)
      - model_name: "claude-opus-4.5"
        litellm_params:
          model: "anthropic/claude-opus-4-5-20251101"
          timeout: 600
        model_info:
          id: "claude-opus-4.5"
          mode: "chat"

      - model_name: "claude-sonnet-4.5"
        litellm_params:
          model: "anthropic/claude-sonnet-4-5-20250929"
          timeout: 300
        model_info:
          id: "claude-sonnet-4.5"
          mode: "chat"

      - model_name: "claude-haiku-4.5"
        litellm_params:
          model: "anthropic/claude-haiku-4-5-20251001"
          timeout: 120
        model_info:
          id: "claude-haiku-4.5"
          mode: "chat"

      # Claude 4 Series
      - model_name: "claude-opus-4"
        litellm_params:
          model: "anthropic/claude-opus-4-20250514"
          timeout: 600
        model_info:
          id: "claude-opus-4"
          mode: "chat"

      - model_name: "claude-sonnet-4"
        litellm_params:
          model: "anthropic/claude-sonnet-4-20250514"
          timeout: 300
        model_info:
          id: "claude-sonnet-4"
          mode: "chat"

      # Legacy aliases
      - model_name: "claude-3-5-sonnet"
        litellm_params:
          model: "anthropic/claude-sonnet-4-5-20250929"
          timeout: 300
        model_info:
          id: "claude-3-5-sonnet"
          mode: "chat"

      - model_name: "claude-3-haiku"
        litellm_params:
          model: "anthropic/claude-3-haiku-20240307"
          timeout: 120
        model_info:
          id: "claude-3-haiku"
          mode: "chat"

      # ===========================================
      # Google Gemini Models (5)
      # ===========================================
      # Docs: https://ai.google.dev/gemini-api/docs/models
      # Requires: GOOGLE_API_KEY or GEMINI_API_KEY

      # Gemini 2.5 Series (Current)
      - model_name: "gemini-2.5-pro"
        litellm_params:
          model: "gemini/gemini-2.5-pro"
          timeout: 300
        model_info:
          id: "gemini-2.5-pro"
          mode: "chat"

      - model_name: "gemini-2.5-flash"
        litellm_params:
          model: "gemini/gemini-2.5-flash"
          timeout: 120
        model_info:
          id: "gemini-2.5-flash"
          mode: "chat"

      - model_name: "gemini-2.5-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.5-flash-lite"
          timeout: 60
        model_info:
          id: "gemini-2.5-flash-lite"
          mode: "chat"

      # Gemini 2.0 Series
      - model_name: "gemini-2.0-flash"
        litellm_params:
          model: "gemini/gemini-2.0-flash"
          timeout: 120
        model_info:
          id: "gemini-2.0-flash"
          mode: "chat"

      - model_name: "gemini-2.0-flash-lite"
        litellm_params:
          model: "gemini/gemini-2.0-flash-lite"
          timeout: 60
        model_info:
          id: "gemini-2.0-flash-lite"
          mode: "chat"

      # ===========================================
      # xAI Grok Models (4)
      # ===========================================
      # Docs: https://docs.x.ai/docs/models
      # Requires: XAI_API_KEY

      # Grok 4 Series (Latest)
      - model_name: "grok-4"
        litellm_params:
          model: "xai/grok-4"
          api_key: "os.environ/XAI_API_KEY"
          timeout: 300
        model_info:
          id: "grok-4"
          mode: "chat"

      - model_name: "grok-4-fast"
        litellm_params:
          model: "xai/grok-4-fast"
          api_key: "os.environ/XAI_API_KEY"
          timeout: 300
        model_info:
          id: "grok-4-fast"
          mode: "chat"

      # Grok 3 Series
      - model_name: "grok-3"
        litellm_params:
          model: "xai/grok-3-beta"
          api_key: "os.environ/XAI_API_KEY"
          timeout: 300
        model_info:
          id: "grok-3"
          mode: "chat"

      - model_name: "grok-3-mini"
        litellm_params:
          model: "xai/grok-3-mini-beta"
          api_key: "os.environ/XAI_API_KEY"
          timeout: 120
        model_info:
          id: "grok-3-mini"
          mode: "chat"

      - model_name: "grok-2"
        litellm_params:
          model: "xai/grok-2-1212"
          api_key: "os.environ/XAI_API_KEY"
          timeout: 300
        model_info:
          id: "grok-2"
          mode: "chat"

      # ===========================================
      # DeepSeek Models (3)
      # ===========================================
      # Docs: https://platform.deepseek.com/docs
      # Requires: DEEPSEEK_API_KEY

      - model_name: "deepseek-v3"
        litellm_params:
          model: "deepseek/deepseek-chat"
          api_key: "os.environ/DEEPSEEK_API_KEY"
          timeout: 300
        model_info:
          id: "deepseek-v3"
          mode: "chat"

      - model_name: "deepseek-r1"
        litellm_params:
          model: "deepseek/deepseek-reasoner"
          api_key: "os.environ/DEEPSEEK_API_KEY"
          timeout: 600
        model_info:
          id: "deepseek-r1"
          mode: "chat"

      - model_name: "deepseek-coder"
        litellm_params:
          model: "deepseek/deepseek-coder"
          api_key: "os.environ/DEEPSEEK_API_KEY"
          timeout: 300
        model_info:
          id: "deepseek-coder"
          mode: "chat"

      # ===========================================
      # AWS Bedrock Models (26)
      # ===========================================
      # Docs: https://docs.litellm.ai/docs/providers/bedrock
      # Requires: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME
      # ~100 serverless models available

      # Anthropic Claude on Bedrock
      - model_name: "bedrock-claude-opus-4.5"
        litellm_params:
          model: "bedrock/anthropic.claude-opus-4-5-20251101-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 600
        model_info:
          id: "bedrock-claude-opus-4.5"
          mode: "chat"

      - model_name: "bedrock-claude-sonnet-4.5"
        litellm_params:
          model: "bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-claude-sonnet-4.5"
          mode: "chat"

      - model_name: "bedrock-claude-haiku-4.5"
        litellm_params:
          model: "bedrock/anthropic.claude-haiku-4-5-20251001-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-claude-haiku-4.5"
          mode: "chat"

      - model_name: "bedrock-claude-sonnet"
        litellm_params:
          model: "bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-claude-sonnet"
          mode: "chat"

      - model_name: "bedrock-claude-haiku"
        litellm_params:
          model: "bedrock/anthropic.claude-3-5-haiku-20241022-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-claude-haiku"
          mode: "chat"

      # Meta Llama on Bedrock
      - model_name: "bedrock-llama-4-405b"
        litellm_params:
          model: "bedrock/meta.llama4-405b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 600
        model_info:
          id: "bedrock-llama-4-405b"
          mode: "chat"

      - model_name: "bedrock-llama-4-70b"
        litellm_params:
          model: "bedrock/meta.llama4-70b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-llama-4-70b"
          mode: "chat"

      - model_name: "bedrock-llama-3.3-70b"
        litellm_params:
          model: "bedrock/meta.llama3-3-70b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-llama-3.3-70b"
          mode: "chat"

      - model_name: "bedrock-llama-3.2-90b"
        litellm_params:
          model: "bedrock/meta.llama3-2-90b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-llama-3.2-90b"
          mode: "chat"

      - model_name: "bedrock-llama-3.2-11b"
        litellm_params:
          model: "bedrock/meta.llama3-2-11b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-llama-3.2-11b"
          mode: "chat"

      - model_name: "bedrock-llama-3.1-70b"
        litellm_params:
          model: "bedrock/meta.llama3-1-70b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-llama-3.1-70b"
          mode: "chat"

      - model_name: "bedrock-llama-3.1-8b"
        litellm_params:
          model: "bedrock/meta.llama3-1-8b-instruct-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-llama-3.1-8b"
          mode: "chat"

      # Mistral on Bedrock
      - model_name: "bedrock-mistral-large-3"
        litellm_params:
          model: "bedrock/mistral.mistral-large-3-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-mistral-large-3"
          mode: "chat"

      - model_name: "bedrock-mistral-large"
        litellm_params:
          model: "bedrock/mistral.mistral-large-2407-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-mistral-large"
          mode: "chat"

      - model_name: "bedrock-ministral-8b"
        litellm_params:
          model: "bedrock/mistral.ministral-8b-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-ministral-8b"
          mode: "chat"

      - model_name: "bedrock-ministral-3b"
        litellm_params:
          model: "bedrock/mistral.ministral-3b-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 60
        model_info:
          id: "bedrock-ministral-3b"
          mode: "chat"

      # Amazon Nova on Bedrock
      - model_name: "bedrock-nova-pro"
        litellm_params:
          model: "bedrock/amazon.nova-pro-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-nova-pro"
          mode: "chat"

      - model_name: "bedrock-nova-lite"
        litellm_params:
          model: "bedrock/amazon.nova-lite-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-nova-lite"
          mode: "chat"

      - model_name: "bedrock-nova-micro"
        litellm_params:
          model: "bedrock/amazon.nova-micro-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 60
        model_info:
          id: "bedrock-nova-micro"
          mode: "chat"

      # Amazon Titan on Bedrock
      - model_name: "bedrock-titan-text-premier"
        litellm_params:
          model: "bedrock/amazon.titan-text-premier-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-titan-text-premier"
          mode: "chat"

      - model_name: "bedrock-titan-text-express"
        litellm_params:
          model: "bedrock/amazon.titan-text-express-v1"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-titan-text-express"
          mode: "chat"

      # DeepSeek on Bedrock
      - model_name: "bedrock-deepseek-r1"
        litellm_params:
          model: "bedrock/deepseek.deepseek-r1-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 600
        model_info:
          id: "bedrock-deepseek-r1"
          mode: "chat"

      # Cohere on Bedrock
      - model_name: "bedrock-cohere-command-r-plus"
        litellm_params:
          model: "bedrock/cohere.command-r-plus-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-cohere-command-r-plus"
          mode: "chat"

      - model_name: "bedrock-cohere-command-r"
        litellm_params:
          model: "bedrock/cohere.command-r-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-cohere-command-r"
          mode: "chat"

      # AI21 on Bedrock
      - model_name: "bedrock-jamba-1.5-large"
        litellm_params:
          model: "bedrock/ai21.jamba-1-5-large-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 300
        model_info:
          id: "bedrock-jamba-1.5-large"
          mode: "chat"

      - model_name: "bedrock-jamba-1.5-mini"
        litellm_params:
          model: "bedrock/ai21.jamba-1-5-mini-v1:0"
          aws_region_name: "ap-south-1"
          timeout: 120
        model_info:
          id: "bedrock-jamba-1.5-mini"
          mode: "chat"

      # ===========================================
      # Google Vertex AI Models (12)
      # ===========================================
      # Docs: https://docs.litellm.ai/docs/providers/vertex
      # Requires: GOOGLE_APPLICATION_CREDENTIALS, VERTEX_PROJECT, VERTEX_LOCATION
      # 200+ models in Model Garden

      # Gemini 2.5 Series (Current)
      - model_name: "vertex-gemini-2.5-pro"
        litellm_params:
          model: "vertex_ai/gemini-2.5-pro"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 300
        model_info:
          id: "vertex-gemini-2.5-pro"
          mode: "chat"

      - model_name: "vertex-gemini-2.5-flash"
        litellm_params:
          model: "vertex_ai/gemini-2.5-flash"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 120
        model_info:
          id: "vertex-gemini-2.5-flash"
          mode: "chat"

      - model_name: "vertex-gemini-2.5-flash-lite"
        litellm_params:
          model: "vertex_ai/gemini-2.5-flash-lite"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 60
        model_info:
          id: "vertex-gemini-2.5-flash-lite"
          mode: "chat"

      # Legacy aliases
      - model_name: "vertex-gemini-pro"
        litellm_params:
          model: "vertex_ai/gemini-2.5-pro"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 300
        model_info:
          id: "vertex-gemini-pro"
          mode: "chat"

      - model_name: "vertex-gemini-flash"
        litellm_params:
          model: "vertex_ai/gemini-2.5-flash"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 120
        model_info:
          id: "vertex-gemini-flash"
          mode: "chat"

      # Anthropic Claude on Vertex AI
      - model_name: "vertex-claude-opus-4.5"
        litellm_params:
          model: "vertex_ai/claude-opus-4-5@20251101"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 600
        model_info:
          id: "vertex-claude-opus-4.5"
          mode: "chat"

      - model_name: "vertex-claude-sonnet-4.5"
        litellm_params:
          model: "vertex_ai/claude-sonnet-4-5@20250929"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 300
        model_info:
          id: "vertex-claude-sonnet-4.5"
          mode: "chat"

      - model_name: "vertex-claude-haiku-4.5"
        litellm_params:
          model: "vertex_ai/claude-haiku-4-5@20251001"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 120
        model_info:
          id: "vertex-claude-haiku-4.5"
          mode: "chat"

      - model_name: "vertex-claude-sonnet"
        litellm_params:
          model: "vertex_ai/claude-3-5-sonnet-v2@20241022"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 300
        model_info:
          id: "vertex-claude-sonnet"
          mode: "chat"

      # DeepSeek on Vertex AI
      - model_name: "vertex-deepseek-v3"
        litellm_params:
          model: "vertex_ai/deepseek-v3.2"
          vertex_project: "os.environ/VERTEX_PROJECT"
          vertex_location: "asia-south1"
          timeout: 300
        model_info:
          id: "vertex-deepseek-v3"
          mode: "chat"

      # ===========================================
      # Self-hosted via Ollama (Local GPU)
      # ===========================================
      # Install: https://ollama.com/download
      # Run: ollama serve && ollama pull llama3.1:8b

      - model_name: "llama-3.1-70b"
        litellm_params:
          model: "ollama/llama3.1:70b"
          api_base: "http://host.docker.internal:11434"
          timeout: 600
        model_info:
          id: "llama-3.1-70b"
          mode: "chat"
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0

      - model_name: "llama-3.1-8b"
        litellm_params:
          model: "ollama/llama3.1:8b"
          api_base: "http://host.docker.internal:11434"
          timeout: 300
        model_info:
          id: "llama-3.1-8b"
          mode: "chat"
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0

      - model_name: "mistral"
        litellm_params:
          model: "ollama/mistral"
          api_base: "http://host.docker.internal:11434"
          timeout: 300
        model_info:
          id: "mistral"
          mode: "chat"
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0

      - model_name: "codellama"
        litellm_params:
          model: "ollama/codellama"
          api_base: "http://host.docker.internal:11434"
          timeout: 300
        model_info:
          id: "codellama"
          mode: "chat"
          input_cost_per_token: 0.0
          output_cost_per_token: 0.0

      # Fallback stubs - show helpful error if Ollama not running
      - model_name: "llama-3.1-70b-fallback"
        litellm_params:
          model: "openai/llama3.1:70b"
          api_base: "http://gpu-stub:8000/v1"
          api_key: "not-needed"
        model_info:
          id: "llama-3.1-70b-fallback"
          mode: "chat"

      - model_name: "llama-3.1-8b-fallback"
        litellm_params:
          model: "openai/llama3.1:8b"
          api_base: "http://gpu-stub:8000/v1"
          api_key: "not-needed"
        model_info:
          id: "llama-3.1-8b-fallback"
          mode: "chat"

    # ===========================================
    # Router Configuration
    # ===========================================
    router_settings:
      routing_strategy: "usage-based-routing"
      routing_strategy_args:
        ttl: 60
        rpm_limit_check: true
        tpm_limit_check: true

      enable_pre_call_checks: true

      retry_policy:
        max_retries: 3
        retry_after_seconds: 1
        exponential_backoff: true

      # Fallback chains (excluding Azure)
      fallbacks:
        # OpenAI fallbacks
        - "gpt-5": ["gpt-5.2", "claude-opus-4.5", "grok-4"]
        - "gpt-5-mini": ["o4-mini", "claude-haiku-4.5", "gemini-2.5-flash"]
        - "o3": ["o3-pro", "gpt-5", "deepseek-r1"]
        - "gpt-4o": ["claude-sonnet-4.5", "grok-3"]
        - "gpt-4o-mini": ["claude-haiku-4.5", "grok-3-mini"]
        # Anthropic fallbacks
        - "claude-opus-4.5": ["claude-sonnet-4.5", "gpt-5", "grok-4"]
        - "claude-sonnet-4.5": ["claude-opus-4.5", "gpt-5", "gemini-2.5-pro"]
        - "claude-haiku-4.5": ["gpt-5-mini", "gemini-2.5-flash"]
        - "claude-3-5-sonnet": ["gpt-4o", "grok-3"]
        - "claude-3-haiku": ["gpt-4o-mini", "grok-3-mini"]
        # Google fallbacks
        - "gemini-2.5-pro": ["gemini-2.5-flash", "claude-sonnet-4.5", "gpt-5"]
        - "gemini-2.5-flash": ["gemini-2.0-flash", "claude-haiku-4.5", "gpt-5-mini"]
        # xAI fallbacks
        - "grok-4": ["grok-3", "gpt-5", "claude-opus-4.5"]
        - "grok-3": ["grok-4", "gpt-4o", "claude-sonnet-4.5"]
        # DeepSeek fallbacks
        - "deepseek-v3": ["deepseek-r1", "gpt-4o", "claude-sonnet-4.5"]
        - "deepseek-r1": ["o3", "deepseek-v3"]
        # Local model fallbacks
        - "llama-3.1-70b": ["llama-3.1-70b-fallback", "bedrock-llama-3.1-70b"]
        - "llama-3.1-8b": ["llama-3.1-8b-fallback", "bedrock-llama-3.1-8b"]

      # Model group aliases for semantic routing (excluding Azure)
      model_group_alias:
        # By capability
        "fast":
          - "gpt-5-mini"
          - "claude-haiku-4.5"
          - "gemini-2.5-flash"
          - "grok-3-mini"
        "smart":
          - "gpt-5"
          - "claude-sonnet-4.5"
          - "gemini-2.5-pro"
          - "grok-4"
        "powerful":
          - "gpt-5.2"
          - "claude-opus-4.5"
          - "o3-pro"
          - "grok-4-fast"
        "reasoning":
          - "o3"
          - "o3-pro"
          - "deepseek-r1"
        "coding":
          - "claude-sonnet-4.5"
          - "deepseek-coder"
          - "codellama"
        "cost-effective":
          - "gpt-5-mini"
          - "claude-haiku-4.5"
          - "gemini-2.5-flash-lite"
          - "deepseek-v3"
        # By provider
        "openai":
          - "gpt-5"
          - "gpt-5.2"
          - "gpt-5-mini"
          - "o3"
          - "o4-mini"
        "anthropic":
          - "claude-opus-4.5"
          - "claude-sonnet-4.5"
          - "claude-haiku-4.5"
        "google":
          - "gemini-2.5-pro"
          - "gemini-2.5-flash"
          - "gemini-2.0-flash"
        "xai":
          - "grok-4"
          - "grok-4-fast"
          - "grok-3"
          - "grok-2"
        "deepseek":
          - "deepseek-v3"
          - "deepseek-r1"
          - "deepseek-coder"
        "bedrock":
          - "bedrock-claude-opus-4.5"
          - "bedrock-claude-sonnet-4.5"
          - "bedrock-claude-haiku-4.5"
          - "bedrock-llama-4-405b"
          - "bedrock-llama-4-70b"
          - "bedrock-llama-3.3-70b"
          - "bedrock-mistral-large-3"
          - "bedrock-nova-pro"
          - "bedrock-deepseek-r1"
          - "bedrock-cohere-command-r-plus"
        "vertex":
          - "vertex-gemini-2.5-pro"
          - "vertex-gemini-2.5-flash"
          - "vertex-claude-opus-4.5"
          - "vertex-claude-sonnet-4.5"
          - "vertex-claude-haiku-4.5"
          - "vertex-deepseek-v3"
        "local":
          - "llama-3.1-70b"
          - "llama-3.1-8b"
          - "mistral"
          - "codellama"

    # ===========================================
    # General Settings
    # ===========================================
    general_settings:
      database_url: "os.environ/DATABASE_URL"
      master_key: "os.environ/LITELLM_MASTER_KEY"

      otel_exporter: "otlp_http"
      otel_endpoint: "http://otel-collector:4318"

      store_model_in_db: false
      background_health_checks: false

    # ===========================================
    # LiteLLM Settings
    # ===========================================
    litellm_settings:
      success_callback: ["otel", "prometheus"]
      failure_callback: ["otel", "prometheus"]
      service_callback: ["prometheus"]
      count_tokens: true
      drop_params: true
      add_function_to_prompt: true
      request_timeout: 300
      stream_timeout: 600

apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: litellm
data:
  config.yaml: |
    # LiteLLM Proxy Configuration
    # Documentation: https://docs.litellm.ai/docs/proxy/configs

    # ===========================================
    # Model Configuration
    # ===========================================
    model_list:
      # Self-hosted models via vLLM (through Agent Gateway)
      - model_name: "llama-3.1-70b"
        litellm_params:
          model: "openai/meta-llama/Llama-3.1-70B-Instruct"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
          api_key: "not-needed-for-vllm"
          custom_llm_provider: "openai"
        model_info:
          id: "llama-3.1-70b"
          mode: "chat"
          input_cost_per_token: 0.0000001  # $0.10 per 1M tokens
          output_cost_per_token: 0.0000003  # $0.30 per 1M tokens
          max_tokens: 131072
          supports_function_calling: true
          supports_vision: false

      - model_name: "llama-3.1-8b"
        litellm_params:
          model: "openai/meta-llama/Llama-3.1-8B-Instruct"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
          api_key: "not-needed-for-vllm"
          custom_llm_provider: "openai"
        model_info:
          id: "llama-3.1-8b"
          mode: "chat"
          input_cost_per_token: 0.00000005  # $0.05 per 1M tokens
          output_cost_per_token: 0.00000015  # $0.15 per 1M tokens
          max_tokens: 131072
          supports_function_calling: true
          supports_vision: false

      # External providers (through Agent Gateway)
      - model_name: "gpt-4o"
        litellm_params:
          model: "openai/gpt-4o"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
        model_info:
          id: "gpt-4o"
          mode: "chat"

      - model_name: "gpt-4o-mini"
        litellm_params:
          model: "openai/gpt-4o-mini"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
        model_info:
          id: "gpt-4o-mini"
          mode: "chat"

      - model_name: "claude-3-5-sonnet"
        litellm_params:
          model: "anthropic/claude-3-5-sonnet-20241022"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
        model_info:
          id: "claude-3-5-sonnet"
          mode: "chat"

      - model_name: "claude-3-haiku"
        litellm_params:
          model: "anthropic/claude-3-haiku-20240307"
          api_base: "http://agentgateway.agentgateway.svc.cluster.local:3000/v1"
        model_info:
          id: "claude-3-haiku"
          mode: "chat"

    # ===========================================
    # Router Configuration
    # ===========================================
    router_settings:
      routing_strategy: "usage-based-routing"
      routing_strategy_args:
        # Prefer self-hosted models when under capacity
        ttl: 60
      enable_pre_call_checks: true
      retry_policy:
        max_retries: 3
        retry_after_seconds: 1
      fallbacks:
        - model_name: "gpt-4o"
          fallback_models:
            - "claude-3-5-sonnet"
            - "llama-3.1-70b"
        - model_name: "claude-3-5-sonnet"
          fallback_models:
            - "gpt-4o"
            - "llama-3.1-70b"
      model_group_alias:
        "fast":
          - "gpt-4o-mini"
          - "claude-3-haiku"
          - "llama-3.1-8b"
        "smart":
          - "gpt-4o"
          - "claude-3-5-sonnet"
          - "llama-3.1-70b"
        "self-hosted":
          - "llama-3.1-70b"
          - "llama-3.1-8b"

    # ===========================================
    # General Settings
    # ===========================================
    general_settings:
      # Database for spend tracking
      database_url: "os.environ/DATABASE_URL"
      master_key: "os.environ/LITELLM_MASTER_KEY"

      # OpenTelemetry
      otel_exporter: "otlp_http"
      otel_endpoint: "http://otel-collector.observability.svc.cluster.local:4318"

      # Request/Response logging
      store_model_in_db: true
      max_request_size_mb: 50
      max_response_size_mb: 50

      # Background jobs
      background_health_checks: true
      health_check_interval: 30

      # Cache settings
      cache: true
      cache_params:
        type: "redis"
        host: "redis.litellm.svc.cluster.local"
        port: 6379
        ttl: 3600  # 1 hour

    # ===========================================
    # Litellm Settings (Cost & Budget)
    # ===========================================
    litellm_settings:
      # Enable cost tracking
      success_callback: ["otel"]
      failure_callback: ["otel"]

      # Token counting
      count_tokens: true

      # Set custom pricing for self-hosted models
      custom_pricing:
        - model: "llama-3.1-70b"
          input_cost_per_token: 0.0000001
          output_cost_per_token: 0.0000003
        - model: "llama-3.1-8b"
          input_cost_per_token: 0.00000005
          output_cost_per_token: 0.00000015

      # Request modifications
      drop_params: true
      add_function_to_prompt: true

    # ===========================================
    # Environment Variables (for documentation)
    # ===========================================
    environment_variables:
      LITELLM_MASTER_KEY: "os.environ/LITELLM_MASTER_KEY"
      DATABASE_URL: "os.environ/DATABASE_URL"
      REDIS_HOST: "os.environ/REDIS_HOST"
      REDIS_PORT: "os.environ/REDIS_PORT"

# LiteLLM Proxy Configuration
# Documentation: https://docs.litellm.ai/docs/proxy/configs
#
# Supported Providers:
# - OpenAI (GPT-5, o3, o4-mini)
# - Anthropic (Claude Opus 4.5, Sonnet 4.5, Haiku 4.5)
# - Google (Gemini 3 Pro, 2.5 Pro/Flash)
# - xAI (Grok 4, Grok 3)
# - DeepSeek (V3.2, R1)
# - AWS Bedrock (~100 models: Claude, Llama 4, Nova, Mistral, Titan, DeepSeek)
# - Google Vertex AI (200+ models: Gemini, Claude, DeepSeek)
# - Azure OpenAI (GPT-5, GPT-4.1, o-series, embeddings, audio)
# - Ollama (local models)

# ===========================================
# Model Configuration
# ===========================================
model_list:

  # ===========================================
  # OpenAI Models
  # ===========================================
  # Docs: https://platform.openai.com/docs/models

  # GPT-5 Series (Flagship)
  - model_name: "gpt-5"
    litellm_params:
      model: "gpt-5"
      timeout: 300
    model_info:
      id: "gpt-5"
      mode: "chat"

  - model_name: "gpt-5.2"
    litellm_params:
      model: "gpt-5.2"
      timeout: 300
    model_info:
      id: "gpt-5.2"
      mode: "chat"

  - model_name: "gpt-5-mini"
    litellm_params:
      model: "gpt-5-mini"
      timeout: 120
    model_info:
      id: "gpt-5-mini"
      mode: "chat"

  # o-Series Reasoning Models
  - model_name: "o3"
    litellm_params:
      model: "o3"
      timeout: 600
    model_info:
      id: "o3"
      mode: "chat"

  - model_name: "o3-pro"
    litellm_params:
      model: "o3-pro"
      timeout: 900
    model_info:
      id: "o3-pro"
      mode: "chat"

  - model_name: "o4-mini"
    litellm_params:
      model: "o4-mini"
      timeout: 300
    model_info:
      id: "o4-mini"
      mode: "chat"

  # Legacy (still supported)
  - model_name: "gpt-4o"
    litellm_params:
      model: "gpt-4o"
      timeout: 300
    model_info:
      id: "gpt-4o"
      mode: "chat"

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "gpt-4o-mini"
      timeout: 120
    model_info:
      id: "gpt-4o-mini"
      mode: "chat"

  # ===========================================
  # Anthropic Models
  # ===========================================
  # Docs: https://docs.anthropic.com/claude/docs/models-overview

  # Claude 4.5 Series (Latest)
  - model_name: "claude-opus-4.5"
    litellm_params:
      model: "claude-opus-4-5-20251101"
      timeout: 600
    model_info:
      id: "claude-opus-4.5"
      mode: "chat"

  - model_name: "claude-sonnet-4.5"
    litellm_params:
      model: "claude-sonnet-4-5-20251101"
      timeout: 300
    model_info:
      id: "claude-sonnet-4.5"
      mode: "chat"

  - model_name: "claude-haiku-4.5"
    litellm_params:
      model: "claude-haiku-4-5-20251101"
      timeout: 120
    model_info:
      id: "claude-haiku-4.5"
      mode: "chat"

  # Claude 4 Series
  - model_name: "claude-opus-4"
    litellm_params:
      model: "claude-opus-4-20250514"
      timeout: 600
    model_info:
      id: "claude-opus-4"
      mode: "chat"

  - model_name: "claude-sonnet-4"
    litellm_params:
      model: "claude-sonnet-4-20250514"
      timeout: 300
    model_info:
      id: "claude-sonnet-4"
      mode: "chat"

  # Legacy aliases
  - model_name: "claude-3-5-sonnet"
    litellm_params:
      model: "claude-sonnet-4-5-20251101"
      timeout: 300
    model_info:
      id: "claude-3-5-sonnet"
      mode: "chat"

  - model_name: "claude-3-haiku"
    litellm_params:
      model: "claude-haiku-4-5-20251101"
      timeout: 120
    model_info:
      id: "claude-3-haiku"
      mode: "chat"

  # ===========================================
  # Google Gemini Models
  # ===========================================
  # Docs: https://ai.google.dev/gemini-api/docs/models
  # Requires: GOOGLE_API_KEY or GEMINI_API_KEY

  # Gemini 3 Series (Latest)
  - model_name: "gemini-3-pro"
    litellm_params:
      model: "gemini/gemini-3-pro"
      timeout: 300
    model_info:
      id: "gemini-3-pro"
      mode: "chat"

  - model_name: "gemini-3-flash"
    litellm_params:
      model: "gemini/gemini-3-flash"
      timeout: 120
    model_info:
      id: "gemini-3-flash"
      mode: "chat"

  # Gemini 2.5 Series
  - model_name: "gemini-2.5-pro"
    litellm_params:
      model: "gemini/gemini-2.5-pro"
      timeout: 300
    model_info:
      id: "gemini-2.5-pro"
      mode: "chat"

  - model_name: "gemini-2.5-flash"
    litellm_params:
      model: "gemini/gemini-2.5-flash"
      timeout: 120
    model_info:
      id: "gemini-2.5-flash"
      mode: "chat"

  - model_name: "gemini-2.5-flash-lite"
    litellm_params:
      model: "gemini/gemini-2.5-flash-lite"
      timeout: 60
    model_info:
      id: "gemini-2.5-flash-lite"
      mode: "chat"

  # ===========================================
  # xAI Grok Models
  # ===========================================
  # Docs: https://docs.x.ai/docs/models
  # Requires: XAI_API_KEY

  # Grok 4 Series (Latest)
  - model_name: "grok-4"
    litellm_params:
      model: "xai/grok-4"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 300
    model_info:
      id: "grok-4"
      mode: "chat"

  - model_name: "grok-4-heavy"
    litellm_params:
      model: "xai/grok-4-heavy"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 600
    model_info:
      id: "grok-4-heavy"
      mode: "chat"

  # Grok 3 Series
  - model_name: "grok-3"
    litellm_params:
      model: "xai/grok-3"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 300
    model_info:
      id: "grok-3"
      mode: "chat"

  - model_name: "grok-3-mini"
    litellm_params:
      model: "xai/grok-3-mini"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 120
    model_info:
      id: "grok-3-mini"
      mode: "chat"

  # ===========================================
  # DeepSeek Models
  # ===========================================
  # Docs: https://platform.deepseek.com/docs
  # Requires: DEEPSEEK_API_KEY

  - model_name: "deepseek-v3"
    litellm_params:
      model: "deepseek/deepseek-chat"
      api_key: "os.environ/DEEPSEEK_API_KEY"
      timeout: 300
    model_info:
      id: "deepseek-v3"
      mode: "chat"

  - model_name: "deepseek-r1"
    litellm_params:
      model: "deepseek/deepseek-reasoner"
      api_key: "os.environ/DEEPSEEK_API_KEY"
      timeout: 600
    model_info:
      id: "deepseek-r1"
      mode: "chat"

  - model_name: "deepseek-coder"
    litellm_params:
      model: "deepseek/deepseek-coder"
      api_key: "os.environ/DEEPSEEK_API_KEY"
      timeout: 300
    model_info:
      id: "deepseek-coder"
      mode: "chat"

  # ===========================================
  # AWS Bedrock Models
  # ===========================================
  # Docs: https://docs.litellm.ai/docs/providers/bedrock
  # Requires: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION_NAME
  # ~100 serverless models available

  # Anthropic Claude on Bedrock
  - model_name: "bedrock-claude-opus-4.5"
    litellm_params:
      model: "bedrock/anthropic.claude-opus-4-5-20251101-v1:0"
      aws_region_name: "us-east-1"
      timeout: 600
    model_info:
      id: "bedrock-claude-opus-4.5"
      mode: "chat"

  - model_name: "bedrock-claude-sonnet-4.5"
    litellm_params:
      model: "bedrock/anthropic.claude-sonnet-4-5-20251101-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-claude-sonnet-4.5"
      mode: "chat"

  - model_name: "bedrock-claude-haiku-4.5"
    litellm_params:
      model: "bedrock/anthropic.claude-haiku-4-5-20251101-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-claude-haiku-4.5"
      mode: "chat"

  - model_name: "bedrock-claude-sonnet"
    litellm_params:
      model: "bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-claude-sonnet"
      mode: "chat"

  - model_name: "bedrock-claude-haiku"
    litellm_params:
      model: "bedrock/anthropic.claude-3-5-haiku-20241022-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-claude-haiku"
      mode: "chat"

  # Meta Llama on Bedrock
  - model_name: "bedrock-llama-4-405b"
    litellm_params:
      model: "bedrock/meta.llama4-405b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 600
    model_info:
      id: "bedrock-llama-4-405b"
      mode: "chat"

  - model_name: "bedrock-llama-4-70b"
    litellm_params:
      model: "bedrock/meta.llama4-70b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-llama-4-70b"
      mode: "chat"

  - model_name: "bedrock-llama-3.3-70b"
    litellm_params:
      model: "bedrock/meta.llama3-3-70b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-llama-3.3-70b"
      mode: "chat"

  - model_name: "bedrock-llama-3.2-90b"
    litellm_params:
      model: "bedrock/meta.llama3-2-90b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-llama-3.2-90b"
      mode: "chat"

  - model_name: "bedrock-llama-3.2-11b"
    litellm_params:
      model: "bedrock/meta.llama3-2-11b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-llama-3.2-11b"
      mode: "chat"

  - model_name: "bedrock-llama-3.1-70b"
    litellm_params:
      model: "bedrock/meta.llama3-1-70b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-llama-3.1-70b"
      mode: "chat"

  - model_name: "bedrock-llama-3.1-8b"
    litellm_params:
      model: "bedrock/meta.llama3-1-8b-instruct-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-llama-3.1-8b"
      mode: "chat"

  # Mistral on Bedrock
  - model_name: "bedrock-mistral-large-3"
    litellm_params:
      model: "bedrock/mistral.mistral-large-3-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-mistral-large-3"
      mode: "chat"

  - model_name: "bedrock-mistral-large"
    litellm_params:
      model: "bedrock/mistral.mistral-large-2407-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-mistral-large"
      mode: "chat"

  - model_name: "bedrock-ministral-8b"
    litellm_params:
      model: "bedrock/mistral.ministral-8b-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-ministral-8b"
      mode: "chat"

  - model_name: "bedrock-ministral-3b"
    litellm_params:
      model: "bedrock/mistral.ministral-3b-v1:0"
      aws_region_name: "us-east-1"
      timeout: 60
    model_info:
      id: "bedrock-ministral-3b"
      mode: "chat"

  # Amazon Nova on Bedrock
  - model_name: "bedrock-nova-pro"
    litellm_params:
      model: "bedrock/amazon.nova-pro-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-nova-pro"
      mode: "chat"

  - model_name: "bedrock-nova-lite"
    litellm_params:
      model: "bedrock/amazon.nova-lite-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-nova-lite"
      mode: "chat"

  - model_name: "bedrock-nova-micro"
    litellm_params:
      model: "bedrock/amazon.nova-micro-v1:0"
      aws_region_name: "us-east-1"
      timeout: 60
    model_info:
      id: "bedrock-nova-micro"
      mode: "chat"

  # Amazon Titan on Bedrock
  - model_name: "bedrock-titan-text-premier"
    litellm_params:
      model: "bedrock/amazon.titan-text-premier-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-titan-text-premier"
      mode: "chat"

  - model_name: "bedrock-titan-text-express"
    litellm_params:
      model: "bedrock/amazon.titan-text-express-v1"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-titan-text-express"
      mode: "chat"

  # DeepSeek on Bedrock
  - model_name: "bedrock-deepseek-r1"
    litellm_params:
      model: "bedrock/deepseek.deepseek-r1-v1:0"
      aws_region_name: "us-east-1"
      timeout: 600
    model_info:
      id: "bedrock-deepseek-r1"
      mode: "chat"

  # Cohere on Bedrock
  - model_name: "bedrock-cohere-command-r-plus"
    litellm_params:
      model: "bedrock/cohere.command-r-plus-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-cohere-command-r-plus"
      mode: "chat"

  - model_name: "bedrock-cohere-command-r"
    litellm_params:
      model: "bedrock/cohere.command-r-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-cohere-command-r"
      mode: "chat"

  # AI21 on Bedrock
  - model_name: "bedrock-jamba-1.5-large"
    litellm_params:
      model: "bedrock/ai21.jamba-1-5-large-v1:0"
      aws_region_name: "us-east-1"
      timeout: 300
    model_info:
      id: "bedrock-jamba-1.5-large"
      mode: "chat"

  - model_name: "bedrock-jamba-1.5-mini"
    litellm_params:
      model: "bedrock/ai21.jamba-1-5-mini-v1:0"
      aws_region_name: "us-east-1"
      timeout: 120
    model_info:
      id: "bedrock-jamba-1.5-mini"
      mode: "chat"

  # ===========================================
  # Google Vertex AI Models
  # ===========================================
  # Docs: https://docs.litellm.ai/docs/providers/vertex
  # Requires: GOOGLE_APPLICATION_CREDENTIALS, VERTEX_PROJECT, VERTEX_LOCATION
  # 200+ models in Model Garden

  # Gemini 3 Series (Preview)
  - model_name: "vertex-gemini-3-pro"
    litellm_params:
      model: "vertex_ai/gemini-3.0-pro"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 300
    model_info:
      id: "vertex-gemini-3-pro"
      mode: "chat"

  - model_name: "vertex-gemini-3-flash"
    litellm_params:
      model: "vertex_ai/gemini-3.0-flash"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 120
    model_info:
      id: "vertex-gemini-3-flash"
      mode: "chat"

  # Gemini 2.5 Series
  - model_name: "vertex-gemini-2.5-pro"
    litellm_params:
      model: "vertex_ai/gemini-2.5-pro"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 300
    model_info:
      id: "vertex-gemini-2.5-pro"
      mode: "chat"

  - model_name: "vertex-gemini-2.5-flash"
    litellm_params:
      model: "vertex_ai/gemini-2.5-flash"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 120
    model_info:
      id: "vertex-gemini-2.5-flash"
      mode: "chat"

  - model_name: "vertex-gemini-2.5-flash-lite"
    litellm_params:
      model: "vertex_ai/gemini-2.5-flash-lite"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 60
    model_info:
      id: "vertex-gemini-2.5-flash-lite"
      mode: "chat"

  # Legacy aliases
  - model_name: "vertex-gemini-pro"
    litellm_params:
      model: "vertex_ai/gemini-2.5-pro"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 300
    model_info:
      id: "vertex-gemini-pro"
      mode: "chat"

  - model_name: "vertex-gemini-flash"
    litellm_params:
      model: "vertex_ai/gemini-2.5-flash"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 120
    model_info:
      id: "vertex-gemini-flash"
      mode: "chat"

  # Anthropic Claude on Vertex AI
  - model_name: "vertex-claude-opus-4.5"
    litellm_params:
      model: "vertex_ai/claude-opus-4-5@20251101"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-east5"
      timeout: 600
    model_info:
      id: "vertex-claude-opus-4.5"
      mode: "chat"

  - model_name: "vertex-claude-haiku-4.5"
    litellm_params:
      model: "vertex_ai/claude-haiku-4-5@20251101"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-east5"
      timeout: 120
    model_info:
      id: "vertex-claude-haiku-4.5"
      mode: "chat"

  - model_name: "vertex-claude-sonnet"
    litellm_params:
      model: "vertex_ai/claude-3-5-sonnet-v2@20241022"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-east5"
      timeout: 300
    model_info:
      id: "vertex-claude-sonnet"
      mode: "chat"

  # DeepSeek on Vertex AI
  - model_name: "vertex-deepseek-v3"
    litellm_params:
      model: "vertex_ai/deepseek-v3.2"
      vertex_project: "os.environ/VERTEX_PROJECT"
      vertex_location: "us-central1"
      timeout: 300
    model_info:
      id: "vertex-deepseek-v3"
      mode: "chat"

  # ===========================================
  # Azure OpenAI Models
  # ===========================================
  # Docs: https://docs.litellm.ai/docs/providers/azure
  # Requires: AZURE_API_KEY, AZURE_API_BASE, AZURE_API_VERSION

  # GPT-5 Series (Registration Required)
  - model_name: "azure-gpt-5.2"
    litellm_params:
      model: "azure/gpt-5.2"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-gpt-5.2"
      mode: "chat"

  - model_name: "azure-gpt-5.1"
    litellm_params:
      model: "azure/gpt-5.1"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-gpt-5.1"
      mode: "chat"

  # GPT-4.1 Series
  - model_name: "azure-gpt-4.1"
    litellm_params:
      model: "azure/gpt-4.1"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-gpt-4.1"
      mode: "chat"

  - model_name: "azure-gpt-4.1-nano"
    litellm_params:
      model: "azure/gpt-4.1-nano"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 120
    model_info:
      id: "azure-gpt-4.1-nano"
      mode: "chat"

  # o-Series Reasoning Models
  - model_name: "azure-o4-mini"
    litellm_params:
      model: "azure/o4-mini"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-o4-mini"
      mode: "chat"

  - model_name: "azure-o3"
    litellm_params:
      model: "azure/o3"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 600
    model_info:
      id: "azure-o3"
      mode: "chat"

  - model_name: "azure-o3-mini"
    litellm_params:
      model: "azure/o3-mini"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-o3-mini"
      mode: "chat"

  - model_name: "azure-o1"
    litellm_params:
      model: "azure/o1"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 600
    model_info:
      id: "azure-o1"
      mode: "chat"

  # GPT-4o Series
  - model_name: "azure-gpt-4o"
    litellm_params:
      model: "azure/gpt-4o"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-gpt-4o"
      mode: "chat"

  - model_name: "azure-gpt-4o-mini"
    litellm_params:
      model: "azure/gpt-4o-mini"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 120
    model_info:
      id: "azure-gpt-4o-mini"
      mode: "chat"

  # Audio Models
  - model_name: "azure-gpt-4o-transcribe"
    litellm_params:
      model: "azure/gpt-4o-transcribe"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 300
    model_info:
      id: "azure-gpt-4o-transcribe"
      mode: "audio_transcription"

  - model_name: "azure-gpt-4o-mini-tts"
    litellm_params:
      model: "azure/gpt-4o-mini-tts"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 120
    model_info:
      id: "azure-gpt-4o-mini-tts"
      mode: "audio_speech"

  # Embeddings
  - model_name: "azure-text-embedding-3-large"
    litellm_params:
      model: "azure/text-embedding-3-large"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 60
    model_info:
      id: "azure-text-embedding-3-large"
      mode: "embedding"

  - model_name: "azure-text-embedding-3-small"
    litellm_params:
      model: "azure/text-embedding-3-small"
      api_base: "os.environ/AZURE_API_BASE"
      api_key: "os.environ/AZURE_API_KEY"
      api_version: "2025-01-01-preview"
      timeout: 60
    model_info:
      id: "azure-text-embedding-3-small"
      mode: "embedding"

  # ===========================================
  # Self-hosted via Ollama (Local GPU)
  # ===========================================
  # Install: https://ollama.com/download
  # Run: ollama serve && ollama pull llama3.1:8b

  - model_name: "llama-3.1-70b"
    litellm_params:
      model: "ollama/llama3.1:70b"
      api_base: "http://host.docker.internal:11434"
      timeout: 600
    model_info:
      id: "llama-3.1-70b"
      mode: "chat"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: "llama-3.1-8b"
    litellm_params:
      model: "ollama/llama3.1:8b"
      api_base: "http://host.docker.internal:11434"
      timeout: 300
    model_info:
      id: "llama-3.1-8b"
      mode: "chat"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: "mistral"
    litellm_params:
      model: "ollama/mistral"
      api_base: "http://host.docker.internal:11434"
      timeout: 300
    model_info:
      id: "mistral"
      mode: "chat"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  - model_name: "codellama"
    litellm_params:
      model: "ollama/codellama"
      api_base: "http://host.docker.internal:11434"
      timeout: 300
    model_info:
      id: "codellama"
      mode: "chat"
      input_cost_per_token: 0.0
      output_cost_per_token: 0.0

  # Fallback stubs - show helpful error if Ollama not running
  - model_name: "llama-3.1-70b-fallback"
    litellm_params:
      model: "openai/llama3.1:70b"
      api_base: "http://gpu-stub:8000/v1"
      api_key: "not-needed"
    model_info:
      id: "llama-3.1-70b-fallback"
      mode: "chat"

  - model_name: "llama-3.1-8b-fallback"
    litellm_params:
      model: "openai/llama3.1:8b"
      api_base: "http://gpu-stub:8000/v1"
      api_key: "not-needed"
    model_info:
      id: "llama-3.1-8b-fallback"
      mode: "chat"

# ===========================================
# Router Configuration
# ===========================================
router_settings:
  routing_strategy: "usage-based-routing"
  routing_strategy_args:
    ttl: 60
    rpm_limit_check: true
    tpm_limit_check: true

  enable_pre_call_checks: true

  retry_policy:
    max_retries: 3
    retry_after_seconds: 1
    exponential_backoff: true

  # Fallback chains
  fallbacks:
    # OpenAI fallbacks
    - "gpt-5": ["gpt-5.2", "claude-opus-4.5", "grok-4"]
    - "gpt-5-mini": ["o4-mini", "claude-haiku-4.5", "gemini-3-flash"]
    - "o3": ["o3-pro", "gpt-5", "deepseek-r1"]
    - "gpt-4o": ["claude-sonnet-4.5", "grok-3"]
    - "gpt-4o-mini": ["claude-haiku-4.5", "grok-3-mini"]
    # Anthropic fallbacks
    - "claude-opus-4.5": ["claude-sonnet-4.5", "gpt-5", "grok-4"]
    - "claude-sonnet-4.5": ["claude-opus-4.5", "gpt-5", "gemini-3-pro"]
    - "claude-haiku-4.5": ["gpt-5-mini", "gemini-3-flash"]
    - "claude-3-5-sonnet": ["gpt-4o", "grok-3"]
    - "claude-3-haiku": ["gpt-4o-mini", "grok-3-mini"]
    # Google fallbacks
    - "gemini-3-pro": ["gemini-2.5-pro", "claude-sonnet-4.5", "gpt-5"]
    - "gemini-3-flash": ["gemini-2.5-flash", "claude-haiku-4.5", "gpt-5-mini"]
    # xAI fallbacks
    - "grok-4": ["grok-3", "gpt-5", "claude-opus-4.5"]
    - "grok-3": ["grok-4", "gpt-4o", "claude-sonnet-4.5"]
    # DeepSeek fallbacks
    - "deepseek-v3": ["deepseek-r1", "gpt-4o", "claude-sonnet-4.5"]
    - "deepseek-r1": ["o3", "deepseek-v3"]
    # Local model fallbacks
    - "llama-3.1-70b": ["llama-3.1-70b-fallback", "bedrock-llama-3.1-70b"]
    - "llama-3.1-8b": ["llama-3.1-8b-fallback", "bedrock-llama-3.1-8b"]

  # Model group aliases for semantic routing
  model_group_alias:
    # By capability
    "fast":
      - "gpt-5-mini"
      - "claude-haiku-4.5"
      - "gemini-3-flash"
      - "grok-3-mini"
    "smart":
      - "gpt-5"
      - "claude-sonnet-4.5"
      - "gemini-3-pro"
      - "grok-4"
    "powerful":
      - "gpt-5.2"
      - "claude-opus-4.5"
      - "o3-pro"
      - "grok-4-heavy"
    "reasoning":
      - "o3"
      - "o3-pro"
      - "deepseek-r1"
    "coding":
      - "claude-sonnet-4.5"
      - "deepseek-coder"
      - "codellama"
    "cost-effective":
      - "gpt-5-mini"
      - "claude-haiku-4.5"
      - "gemini-2.5-flash-lite"
      - "deepseek-v3"
    # By provider
    "openai":
      - "gpt-5"
      - "gpt-5.2"
      - "gpt-5-mini"
      - "o3"
      - "o4-mini"
    "anthropic":
      - "claude-opus-4.5"
      - "claude-sonnet-4.5"
      - "claude-haiku-4.5"
    "google":
      - "gemini-3-pro"
      - "gemini-3-flash"
      - "gemini-2.5-pro"
    "xai":
      - "grok-4"
      - "grok-4-heavy"
      - "grok-3"
    "deepseek":
      - "deepseek-v3"
      - "deepseek-r1"
      - "deepseek-coder"
    "bedrock":
      - "bedrock-claude-opus-4.5"
      - "bedrock-claude-sonnet-4.5"
      - "bedrock-claude-haiku-4.5"
      - "bedrock-llama-4-405b"
      - "bedrock-llama-4-70b"
      - "bedrock-llama-3.3-70b"
      - "bedrock-mistral-large-3"
      - "bedrock-nova-pro"
      - "bedrock-deepseek-r1"
      - "bedrock-cohere-command-r-plus"
    "vertex":
      - "vertex-gemini-3-pro"
      - "vertex-gemini-3-flash"
      - "vertex-gemini-2.5-pro"
      - "vertex-gemini-2.5-flash"
      - "vertex-claude-opus-4.5"
      - "vertex-claude-haiku-4.5"
      - "vertex-deepseek-v3"
    "azure":
      - "azure-gpt-5.2"
      - "azure-gpt-5.1"
      - "azure-gpt-4.1"
      - "azure-o4-mini"
      - "azure-o3"
      - "azure-o3-mini"
      - "azure-gpt-4o"
      - "azure-gpt-4o-mini"
    "local":
      - "llama-3.1-70b"
      - "llama-3.1-8b"
      - "mistral"
      - "codellama"

# ===========================================
# General Settings
# ===========================================
general_settings:
  database_url: "os.environ/DATABASE_URL"
  master_key: "os.environ/LITELLM_MASTER_KEY"

  otel_exporter: "otlp_http"
  otel_endpoint: "http://otel-collector:4318"

  store_model_in_db: true
  max_request_size_mb: 50
  max_response_size_mb: 50

  # Health checks - every 2 hours to save API costs
  background_health_checks: true
  health_check_interval: 7200

  cache: true
  cache_params:
    type: "redis"
    host: "redis"
    port: 6379
    ttl: 3600
    namespace: "litellm"

# ===========================================
# LiteLLM Settings
# ===========================================
litellm_settings:
  success_callback: ["otel", "prometheus"]
  failure_callback: ["otel", "prometheus"]
  service_callback: ["prometheus"]

  count_tokens: true
  drop_params: true
  add_function_to_prompt: true

  request_timeout: 300
  stream_timeout: 600

# ===========================================
# Budget Configuration
# ===========================================
budget_config:
  global_budget:
    soft_budget: 1000.00
    max_budget: 1500.00
    budget_duration: "monthly"

  default_key_config:
    max_budget: 100.00
    budget_duration: "monthly"
    rpm_limit: 100
    tpm_limit: 100000

# LiteLLM Proxy Configuration
# This is the standalone config file that can be used for local development
# or mounted as a ConfigMap in Kubernetes
# Documentation: https://docs.litellm.ai/docs/proxy/configs

# ===========================================
# Model Configuration
# ===========================================
model_list:
  # Self-hosted models via vLLM (through Agent Gateway)
  - model_name: "llama-3.1-70b"
    litellm_params:
      model: "openai/meta-llama/Llama-3.1-70B-Instruct"
      api_base: "http://agentgateway:9000/v1"
      api_key: "not-needed-for-vllm"
      custom_llm_provider: "openai"
      timeout: 600  # 10 minutes for large model
      stream_timeout: 600
    model_info:
      id: "llama-3.1-70b"
      mode: "chat"
      input_cost_per_token: 0.0000001  # $0.10 per 1M tokens (infrastructure cost)
      output_cost_per_token: 0.0000003  # $0.30 per 1M tokens
      max_tokens: 131072
      supports_function_calling: true
      supports_vision: false
      supports_parallel_function_calling: true

  - model_name: "llama-3.1-8b"
    litellm_params:
      model: "openai/meta-llama/Llama-3.1-8B-Instruct"
      api_base: "http://agentgateway:9000/v1"
      api_key: "not-needed-for-vllm"
      custom_llm_provider: "openai"
      timeout: 300
      stream_timeout: 300
    model_info:
      id: "llama-3.1-8b"
      mode: "chat"
      input_cost_per_token: 0.00000005  # $0.05 per 1M tokens
      output_cost_per_token: 0.00000015  # $0.15 per 1M tokens
      max_tokens: 131072
      supports_function_calling: true
      supports_vision: false

  # OpenAI models (direct API for local dev)
  - model_name: "gpt-4o"
    litellm_params:
      model: "gpt-4o"
      timeout: 300
    model_info:
      id: "gpt-4o"
      mode: "chat"

  - model_name: "gpt-4o-mini"
    litellm_params:
      model: "gpt-4o-mini"
      timeout: 120
    model_info:
      id: "gpt-4o-mini"
      mode: "chat"

  - model_name: "gpt-4-turbo"
    litellm_params:
      model: "gpt-4-turbo"
      timeout: 300
    model_info:
      id: "gpt-4-turbo"
      mode: "chat"

  # Anthropic models (direct API)
  - model_name: "claude-3-5-sonnet"
    litellm_params:
      model: "claude-3-5-sonnet-20241022"
      timeout: 300
    model_info:
      id: "claude-3-5-sonnet"
      mode: "chat"

  - model_name: "claude-3-opus"
    litellm_params:
      model: "claude-3-opus-20240229"
      timeout: 600
    model_info:
      id: "claude-3-opus"
      mode: "chat"

  - model_name: "claude-3-haiku"
    litellm_params:
      model: "claude-3-haiku-20240307"
      timeout: 120
    model_info:
      id: "claude-3-haiku"
      mode: "chat"

  # XAI (Grok) models - use grok-3 (current)
  - model_name: "grok-3"
    litellm_params:
      model: "xai/grok-3"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 300
    model_info:
      id: "grok-3"
      mode: "chat"

  - model_name: "grok-3-mini"
    litellm_params:
      model: "xai/grok-3-mini"
      api_key: "os.environ/XAI_API_KEY"
      timeout: 120
    model_info:
      id: "grok-3-mini"
      mode: "chat"

# ===========================================
# Router Configuration
# ===========================================
router_settings:
  routing_strategy: "usage-based-routing"
  routing_strategy_args:
    ttl: 60
    rpm_limit_check: true
    tpm_limit_check: true

  enable_pre_call_checks: true

  # Retry configuration
  retry_policy:
    max_retries: 3
    retry_after_seconds: 1
    exponential_backoff: true

  # Fallback configuration (new format)
  fallbacks:
    - "gpt-4o": ["claude-3-5-sonnet", "grok-3"]
    - "claude-3-5-sonnet": ["gpt-4o", "grok-3"]
    - "gpt-4o-mini": ["claude-3-haiku", "grok-3-mini"]
    - "claude-3-haiku": ["gpt-4o-mini", "grok-3-mini"]

  # Model group aliases for semantic routing
  model_group_alias:
    "fast":
      - "gpt-4o-mini"
      - "claude-3-haiku"
      - "grok-3-mini"
    "smart":
      - "gpt-4o"
      - "claude-3-5-sonnet"
      - "grok-3"
    "powerful":
      - "claude-3-opus"
      - "gpt-4-turbo"
    "xai":
      - "grok-3"
      - "grok-3-mini"
    "openai":
      - "gpt-4o"
      - "gpt-4o-mini"
      - "gpt-4-turbo"
    "anthropic":
      - "claude-3-5-sonnet"
      - "claude-3-opus"
      - "claude-3-haiku"
    "cost-effective":
      - "gpt-4o-mini"
      - "claude-3-haiku"
      - "grok-3-mini"

# ===========================================
# General Settings
# ===========================================
general_settings:
  # Database configuration
  database_url: "os.environ/DATABASE_URL"
  master_key: "os.environ/LITELLM_MASTER_KEY"

  # OpenTelemetry configuration
  otel_exporter: "otlp_http"
  otel_endpoint: "http://otel-collector:4318"

  # Logging
  store_model_in_db: true
  max_request_size_mb: 50
  max_response_size_mb: 50

  # Health checks - every 2 hours to save costs (7200 seconds)
  background_health_checks: true
  health_check_interval: 7200

  # Cache configuration
  cache: true
  cache_params:
    type: "redis"
    host: "redis"
    port: 6379
    ttl: 3600
    namespace: "litellm"

# ===========================================
# LiteLLM Settings
# ===========================================
litellm_settings:
  # Callbacks - include prometheus for metrics
  success_callback: ["otel", "prometheus"]
  failure_callback: ["otel", "prometheus"]
  service_callback: ["prometheus"]

  # Token counting
  count_tokens: true

  # Custom pricing for self-hosted models
  custom_pricing:
    - model: "llama-3.1-70b"
      input_cost_per_token: 0.0000001
      output_cost_per_token: 0.0000003
    - model: "llama-3.1-8b"
      input_cost_per_token: 0.00000005
      output_cost_per_token: 0.00000015

  # Request handling
  drop_params: true
  add_function_to_prompt: true

  # Timeout defaults
  request_timeout: 300
  stream_timeout: 600

# ===========================================
# Budget & Rate Limiting (Examples)
# ===========================================
# Note: In production, these are typically managed via API
# but can be seeded here for initial setup

# Example budget configuration
budget_config:
  # Global budget limits
  global_budget:
    soft_budget: 1000.00  # $1000 soft limit (warning)
    max_budget: 1500.00   # $1500 hard limit (reject)
    budget_duration: "monthly"

  # Default limits for new keys
  default_key_config:
    max_budget: 100.00
    budget_duration: "monthly"
    rpm_limit: 100
    tpm_limit: 100000

# Example team budgets (seeded via API in production)
team_budgets:
  - team_id: "engineering"
    max_budget: 5000.00
    budget_duration: "monthly"
    models:
      - "gpt-4o"
      - "claude-3-5-sonnet"
      - "llama-3.1-70b"
      - "llama-3.1-8b"

  - team_id: "data-science"
    max_budget: 10000.00
    budget_duration: "monthly"
    models:
      - "gpt-4o"
      - "claude-3-opus"
      - "llama-3.1-70b"

  - team_id: "product"
    max_budget: 2000.00
    budget_duration: "monthly"
    models:
      - "gpt-4o-mini"
      - "claude-3-haiku"
      - "llama-3.1-8b"
